---
layout: default
title: "About This"
---

I'm a cognitive science researcher working for the Cognitive Data Science (CODAS) Lab at Rutgers University - Newark. Our research broadly covers computational analysis of human behavior, including probabilistic models of the visual system, artificially intelligent teaching agents for educational applications, modeling of learning curves in early childhood, and application of computer vision towards understanding group dynamics.

I examine human behavior in four primary ways:

## Attention, Memory and Awareness

Years of research in biology, psychology and neuroscience have demonstrated the limited scope of vision. The majority of visual processing occurs in the fovea, a concentrated patch of light receptors near the center of the retina that covers roughly the size of a thumbnail at arm's distance. If the visual system is so limited, why does the world around us seem complete? The visual system achieves these feats using *visual attention* to track informative properties and *short term memory* to briefly store items of potential impact to the near future. Many of my experiments investigate situational awareness, often demonstrating flagrant inabilities to notice moving vehicles, parts of conversation, or even items held in one's own hands.

Recently, I've become interested in the low-level visual properties that direct attention seemingly automatically. To this end, we are using virtual reality training to increase awareness of specific types of visual stimuli. The end goal will be to improve people's perceptual awareness while they go about their day. Imagine: training your brain to detect birds like a professional birdwatcher while you do nothing more than watching TV. That's the goal, anyway.

## Social Attention

How do we know what other people are thinking?  Mind-reading is a frequent topic of science fiction, but the truth is that human beings participating in something closer to "mind-guessing" every day. Say that you hear someone scream. You turn around and you see your roommate on a stool looking down. You don't have to think at all before you know that something on the floor has surprised him, and it's not long before you see what he sees: a mouse scurrying past.

Human history is filled with people failing to understand other people. However, we can seamlessly direct others to the focus of our attention using eye movements, body position, gesture and language. Some of my research looks at the effect of social stimuli on attention. On a basic science approach, we're interested in the earliest stages of brain activation that lead towards a mature theory of mind (the *nearly* unique human ability to reason about other minds), which in turn might be used for assessing autism and other mental disorders characterized by a theory of mind deficit. On an applied setting, I've used insights on social attention to improve readability of computer generated narratives and facilitate attention in multiple object tracking situations. I'm also currently working with the Rutgers Child Study Center to develop machine-learning algorithms for the recognition of social-emotional information from faces. Recently, we <a href="https://github.com/ljbaker/ljbaker.github.io/tree/master/face_cat_experiment">crowdsourced emotional categorization to workers on Amazon Mechanical Turk</a> to get an idea of how our algorithm compared to human performance while also getting additional training images for future studies.

## Event Perception

With a glance you can understand so much of the world around you. The brain recognizes collections of features (e.g., *red*, *smooth*, *shiny*) as objects (*an apple*). The brain also automatically encodes collections of objects in space (e.g., *lawn chairs*, *sand*, *plastic buckets*, *sunshine*) as a single scene (*a beach*). Likewise, the brain encodes whole moments of time (e.g., *pouring coffee*, *grabbing sugar*, *pouring surgar*, *grabbing milk*, *pouring milk*, *stirring*) as a single event (*making coffee*). I spent a lot of time trying to understand how we understand events.

Research in event perception has extremely useful scientific and real-world applications. Within basic science, understanding how the brain processes events reveals a great deal about how systems of attention, memory and decision-making leverage our limited mental capacities to focus on important items while ignoring the banal. In application, understanding how the brain parses events can lead to innovations in condensing information. Think about it: automatic video encoding that extracts only the moments most important for understanding the event, or software that leverages how human beings anticipate events to predict short-term human behavior. All these things are very possible within our lifetime.

## Data Science and Statistical Analysis

I really enjoy digging through data. I have a doctoral certificate in quantitative methods and postdoctoral training in machine learning, which I use to find nuanced trends in human behavior using massive collections of data. I will occasionally post <a href="https://github.com/ljbaker/ljbaker.github.io/tree/master/CogSci17">data analysis from completed projects</a> and welcome feedback on more efficient or interesting methods.
