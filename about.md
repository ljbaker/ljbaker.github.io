---
layout: default
title: "About This"
---

 I specialize in machine learning algorithms for human behavior. At <a href="https://www.pymetrics.com/">pymetrics</a>, these AI applications lead to two important (and fascinating) considerations. First, algorithms dealing with human behavior will often be *biased* against some demographic, be it racial, gender, socio-economic or regional, and the future of AI lies in estimating and curbing this algorithmic bias. Second, many significant contributors to AI rely on blackbox algorithms, but decisions that impact human lives require transparency to meet any number of legal and ethical standards. My work at pymetrics revolves around these issues of transparency and equality while still creating algorithms that accurately predict human behavior. It's crazy fun.

# Machine Learning Research at pymetrics

<stuff goes here>

# Academic Research

I received a PhD in Psychology from the Seeing and Thinking Lab at Vanderbilt University and worked as a post-doctoral researcher in the Cognitive Data Science (CODAS) Lab at Rutgers University - Newark. My research has evolved from investigating the cognitive mechanisms governing attention and memory in everyday events to broader algorithmic modeling of cross-cultural human cognition. That being said, I'll nerd out over just about anything.

My research broadly covers computational analysis of human behavior, including probabilistic models of the visual system, artificially intelligent teaching agents for educational applications, modeling of learning curves in early childhood, and application of computer vision towards understanding group dynamics. This can be summarized in three primary subjects:

## Attention, Memory and Awareness

Years of research in biology, psychology and neuroscience have demonstrated the limited scope of vision. The majority of visual processing occurs in the fovea, a concentrated patch of light receptors near the center of the retina that covers roughly the size of a thumbnail at arm's distance. If the visual system is so limited, why does the world around us seem complete? The visual system achieves these feats using *visual attention* to track informative properties and *short term memory* to briefly store items of potential impact to the near future. Many of my experiments investigate situational awareness, often demonstrating flagrant inabilities to notice moving vehicles, parts of conversation, or even items held in one's own hands.

Recently, I've become interested in the low-level visual properties that direct attention seemingly automatically. To this end, we are using virtual reality training to increase awareness of specific types of visual stimuli. The end goal will be to improve people's perceptual awareness while they go about their day. Imagine: training your brain to detect birds like a professional birdwatcher while you do nothing more than watching TV. That's the goal, anyway.

## Social Attention

How do we know what other people are thinking?  Mind-reading is a frequent topic of science fiction, but the truth is that human beings participating in something closer to "mind-guessing" every day. Say that you hear someone scream. You turn around and you see your roommate on a stool looking down. You don't have to think at all before you know that something on the floor has surprised him, and it's not long before you see what he sees: a mouse scurrying past.

Human history is filled with people failing to understand other people. However, we can seamlessly direct others to the focus of our attention using eye movements, body position, gesture and language. Some of my research looks at the effect of social stimuli on attention. On a basic science approach, we're interested in the earliest stages of brain activation that lead towards a mature theory of mind (the *nearly* unique human ability to reason about other minds), which in turn might be used for assessing autism and other mental disorders characterized by a theory of mind deficit. On an applied setting, I've used insights on social attention to improve readability of computer generated narratives and facilitate attention in multiple object tracking situations. I'm also currently working with the Rutgers Child Study Center to develop machine-learning algorithms for the recognition of social-emotional information from faces. Recently, we <a href="https://github.com/ljbaker/ljbaker.github.io/tree/master/face_cat_experiment">crowdsourced emotional categorization to workers on Amazon Mechanical Turk</a> to get an idea of how our algorithm compared to human performance while also getting additional training images for future studies.

## Event Perception

With a glance you can understand so much of the world around you. The brain recognizes collections of features (e.g., *red*, *smooth*, *shiny*) as objects (*an apple*). The brain also automatically encodes collections of objects in space (e.g., *lawn chairs*, *sand*, *plastic buckets*, *sunshine*) as a single scene (*a beach*). Likewise, the brain encodes whole moments of time (e.g., *pouring coffee*, *grabbing sugar*, *pouring surgar*, *grabbing milk*, *pouring milk*, *stirring*) as a single event (*making coffee*). I spent a lot of time trying to understand how we understand events.

Research in event perception has extremely useful scientific and real-world applications. Within basic science, understanding how the brain processes events reveals a great deal about how systems of attention, memory and decision-making leverage our limited mental capacities to focus on important items while ignoring the banal. In application, understanding how the brain parses events can lead to innovations in condensing information. Think about it: automatic video encoding that extracts only the moments most important for understanding the event, or software that leverages how human beings anticipate events to predict short-term human behavior. All these things are very possible within our lifetime.
